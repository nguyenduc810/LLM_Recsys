{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 16:01:44.768555: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-19 16:01:44.828403: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-19 16:01:44.828441: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-19 16:01:44.828462: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-19 16:01:44.835398: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import os \n",
    "from pprint import pprint\n",
    "import json \n",
    "import pandas as pd \n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(idir):\n",
    "    if not os.path.isdir(idir):\n",
    "        os.makedirs(idir)\n",
    "\n",
    "def get_noun_phrases(doc, output=None, keep=None):\n",
    "    if keep is None:\n",
    "        return list([np.text.lower() for np in doc.noun_chunks])\n",
    "    if output is None:\n",
    "        output = {}\n",
    "    kws = []\n",
    "    for nc in doc.noun_chunks:\n",
    "        ws = []\n",
    "        for word in nc:\n",
    "            if word.pos_ in keep:\n",
    "                ws.append(word.text.lower())\n",
    "        if len(ws) > 0:\n",
    "            n = ' '.join(ws)\n",
    "            output[n] = output.get(n, 0) + 1\n",
    "            kws.append(n)\n",
    "    return output, kws \n",
    "\n",
    "def increase_count(idict, key, freq):\n",
    "    if key not in idict:\n",
    "        idict[key] = 0\n",
    "    idict[key] += freq\n",
    "\n",
    "\n",
    "def add_to_dict(idict, key, value, freq=1):\n",
    "    if key not in idict:\n",
    "        idict[key] = {}\n",
    "    if value not in idict[key]:\n",
    "        idict[key][value] = 0\n",
    "    idict[key][value] += freq\n",
    "\n",
    "\n",
    "def get_unique_values(idict, count_only=False):\n",
    "    if count_only:\n",
    "        return {k: len(set(v)) for k, v in idict.items()}\n",
    "    else:\n",
    "        return {k: list(set(v)) for k, v in idict.items()}\n",
    "\n",
    "\n",
    "def save_np_info(np2count, np2reviews, np2rest, np2users, ofile, count_only=False):\n",
    "    # output = {\"np2count\": np2count, \"np2review_count\": count_unique_values(np2reviews), \n",
    "    #         'np2res_count': count_unique_values(np2rest), 'np2user_count': count_unique_values(np2users)}\n",
    "    output = {\"np2count\": np2count, \"np2reviews\": np2reviews, \n",
    "            'np2rests': np2rest, 'np2users': np2users}\n",
    "    json.dump(output, open(ofile, 'w'))\n",
    "    print(\"Saved to\", ofile)\n",
    "\n",
    "def extract_raw_keywords_for_reviews(data, ofile, \n",
    "                                     keep=['ADJ', 'NOUN', 'PROPN', 'VERB'], overwrite=False, \n",
    "                                     review2keyword_ofile=None):\n",
    "    if os.path.isfile(ofile) and not overwrite:\n",
    "        print(\"Existing output file. Stop! (set overwrite=True to overwrite)\")\n",
    "        return \n",
    "    np2count = {}   # frequency \n",
    "    np2review2count = {}  # reviews \n",
    "    np2rest2count = {}  # \n",
    "    np2user2count = {} \n",
    "    counter = 0\n",
    "    review2keywords = {}\n",
    "    for rid, uid, restid, text in tqdm(zip(data['review_id'], data['user_id'], data['rest_id'], data['text']), total=len(data)):\n",
    "        doc = nlp(text)\n",
    "        tmp, keywords = get_noun_phrases(doc, keep=keep)  # np for this review \n",
    "        for np, freq in tmp.items():\n",
    "            increase_count(np2count, np, freq)\n",
    "            add_to_dict(np2review2count, np, rid, freq)\n",
    "            add_to_dict(np2rest2count, np, restid, freq)\n",
    "            add_to_dict(np2user2count, np, uid, freq)\n",
    "        review2keywords[rid] = keywords\n",
    "        # counter += 1\n",
    "        # if counter % 2 == 0:\n",
    "            # save_np_info(np2count, np2review2count, np2rest2count, np2user2count, ofile)\n",
    "    save_np_info(np2count, np2review2count, np2rest2count, np2user2count, ofile)\n",
    "    if review2keyword_ofile is not None: \n",
    "        df = pd.DataFrame({\"Review_ID\": list(review2keywords.keys()), \"Keywords\": list(review2keywords.values())})\n",
    "        df.to_csv(review2keyword_ofile)\n",
    "\n",
    "\n",
    "def load_split(sfile='/home/ubuntu/duc.nm195858/keyext_LLM/splits.json', city='singapore', setname='train'):\n",
    "    return json.load(open(sfile))[city][setname]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract keywords for train set\n",
    "- Extract keywords from train set \n",
    "- Then use postprocessing for choosing a subset of keywords \n",
    "- Use these keywords for test, dev? (maybe don't need, but can do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for edinburgh test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1970/1970 [01:16<00:00, 25.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /home/ubuntu/duc.nm195858/keyext_LLM/preprocessed/by_city-users_min_3_reviews/keywords_spacy/test/edinburgh-keywords.json\n"
     ]
    }
   ],
   "source": [
    "# setname = 'test'\n",
    "CITIES =['edinburgh'] #['charlotte', 'edinburgh', 'lasvegas', 'london', 'phoenix', 'pittsburgh', 'singapore']\n",
    "sets = ['test']#, 'test', 'dev']\n",
    "for city in CITIES: \n",
    "    dt = pd.read_csv('/home/ubuntu/duc.nm195858/keyext_LLM/reviews/{}.csv'.format(city))\n",
    "    for setname in sets: \n",
    "        print(\"Processing for\", city, setname)\n",
    "        uids = load_split(city=city, setname=setname)\n",
    "        dt_set = dt[dt['user_id'].isin(uids)]\n",
    "        odir = '/home/ubuntu/duc.nm195858/keyext_LLM/preprocessed/by_city-users_min_3_reviews/keywords_spacy/' + setname\n",
    "        mkdir(odir)\n",
    "        extract_raw_keywords_for_reviews(dt_set, ofile=os.path.join(odir, city + '-keywords.json'), keep=['ADJ', 'NOUN', 'PROPN', 'VERB'], \n",
    "                                        overwrite=True, review2keyword_ofile=os.path.join(odir,city+\"-review2keywords.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
