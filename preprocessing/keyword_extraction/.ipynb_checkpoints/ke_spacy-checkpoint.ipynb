{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os \n",
    "from pprint import pprint\n",
    "import json \n",
    "import pandas as pd \n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(idir):\n",
    "    if not os.path.isdir(idir):\n",
    "        os.makedirs(idir)\n",
    "\n",
    "def get_noun_phrases(doc, output=None, keep=None):\n",
    "    if keep is None:\n",
    "        return list([np.text.lower() for np in doc.noun_chunks])\n",
    "    if output is None:\n",
    "        output = {}\n",
    "    kws = []\n",
    "    for nc in doc.noun_chunks:\n",
    "        ws = []\n",
    "        for word in nc:\n",
    "            if word.pos_ in keep:\n",
    "                ws.append(word.text.lower())\n",
    "        if len(ws) > 0:\n",
    "            n = ' '.join(ws)\n",
    "            output[n] = output.get(n, 0) + 1\n",
    "            kws.append(n)\n",
    "    return output, kws \n",
    "\n",
    "def increase_count(idict, key, freq):\n",
    "    if key not in idict:\n",
    "        idict[key] = 0\n",
    "    idict[key] += freq\n",
    "\n",
    "\n",
    "def add_to_dict(idict, key, value, freq=1):\n",
    "    if key not in idict:\n",
    "        idict[key] = {}\n",
    "    if value not in idict[key]:\n",
    "        idict[key][value] = 0\n",
    "    idict[key][value] += freq\n",
    "\n",
    "\n",
    "def get_unique_values(idict, count_only=False):\n",
    "    if count_only:\n",
    "        return {k: len(set(v)) for k, v in idict.items()}\n",
    "    else:\n",
    "        return {k: list(set(v)) for k, v in idict.items()}\n",
    "\n",
    "\n",
    "def save_np_info(np2count, np2reviews, np2rest, np2users, ofile, count_only=False):\n",
    "    # output = {\"np2count\": np2count, \"np2review_count\": count_unique_values(np2reviews), \n",
    "    #         'np2res_count': count_unique_values(np2rest), 'np2user_count': count_unique_values(np2users)}\n",
    "    output = {\"np2count\": np2count, \"np2reviews\": np2reviews, \n",
    "            'np2rests': np2rest, 'np2users': np2users}\n",
    "    json.dump(output, open(ofile, 'w'))\n",
    "    print(\"Saved to\", ofile)\n",
    "\n",
    "def extract_raw_keywords_for_reviews(data, ofile, \n",
    "                                     keep=['ADJ', 'NOUN', 'PROPN', 'VERB'], overwrite=False, \n",
    "                                     review2keyword_ofile=None):\n",
    "    if os.path.isfile(ofile) and not overwrite:\n",
    "        print(\"Existing output file. Stop! (set overwrite=True to overwrite)\")\n",
    "        return \n",
    "    np2count = {}   # frequency \n",
    "    np2review2count = {}  # reviews \n",
    "    np2rest2count = {}  # \n",
    "    np2user2count = {} \n",
    "    counter = 0\n",
    "    review2keywords = {}\n",
    "    for rid, uid, restid, text in tqdm(zip(data['review_id'], data['user_id'], data['rest_id'], data['text']), total=len(data)):\n",
    "        doc = nlp(text)\n",
    "        tmp, keywords = get_noun_phrases(doc, keep=keep)  # np for this review \n",
    "        for np, freq in tmp.items():\n",
    "            increase_count(np2count, np, freq)\n",
    "            add_to_dict(np2review2count, np, rid, freq)\n",
    "            add_to_dict(np2rest2count, np, restid, freq)\n",
    "            add_to_dict(np2user2count, np, uid, freq)\n",
    "        review2keywords[rid] = keywords\n",
    "        # counter += 1\n",
    "        # if counter % 2 == 0:\n",
    "            # save_np_info(np2count, np2review2count, np2rest2count, np2user2count, ofile)\n",
    "    save_np_info(np2count, np2review2count, np2rest2count, np2user2count, ofile)\n",
    "    if review2keyword_ofile is not None: \n",
    "        df = pd.DataFrame({\"Review_ID\": list(review2keywords.keys()), \"Keywords\": list(review2keywords.values())})\n",
    "        df.to_csv(review2keyword_ofile)\n",
    "\n",
    "\n",
    "def load_split(sfile='../../../data/preprocessed/splits.json', city='singapore', setname='train'):\n",
    "    return json.load(open(sfile))[city][setname]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract keywords for train set\n",
    "- Extract keywords from train set \n",
    "- Then use postprocessing for choosing a subset of keywords \n",
    "- Use these keywords for test, dev? (maybe don't need, but can do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for charlotte train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90426/90426 [25:51<00:00, 58.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../../data/preprocessed/by_city-users_min_3_reviews/keywords_spacy/train/charlotte-keywords.json\n",
      "Processing for charlotte test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16735/16735 [04:52<00:00, 57.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../../data/preprocessed/by_city-users_min_3_reviews/keywords_spacy/test/charlotte-keywords.json\n",
      "Processing for charlotte dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5611/5611 [01:33<00:00, 59.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../../data/preprocessed/by_city-users_min_3_reviews/keywords_spacy/dev/charlotte-keywords.json\n",
      "Processing for edinburgh train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10342/10342 [03:37<00:00, 47.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../../data/preprocessed/by_city-users_min_3_reviews/keywords_spacy/train/edinburgh-keywords.json\n",
      "Processing for edinburgh test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1970/1970 [00:42<00:00, 46.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../../data/preprocessed/by_city-users_min_3_reviews/keywords_spacy/test/edinburgh-keywords.json\n",
      "Processing for edinburgh dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 441/441 [00:08<00:00, 52.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../../data/preprocessed/by_city-users_min_3_reviews/keywords_spacy/dev/edinburgh-keywords.json\n",
      "Processing for lasvegas train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 343524/343524 [1:44:00<00:00, 55.05it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../../data/preprocessed/by_city-users_min_3_reviews/keywords_spacy/train/lasvegas-keywords.json\n",
      "Processing for lasvegas test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64686/64686 [19:16<00:00, 55.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../../data/preprocessed/by_city-users_min_3_reviews/keywords_spacy/test/lasvegas-keywords.json\n",
      "Processing for lasvegas dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20572/20572 [05:56<00:00, 57.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../../data/preprocessed/by_city-users_min_3_reviews/keywords_spacy/dev/lasvegas-keywords.json\n",
      "Processing for london train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33990/33990 [11:20<00:00, 49.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../../data/preprocessed/by_city-users_min_3_reviews/keywords_spacy/train/london-keywords.json\n",
      "Processing for london test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6222/6222 [02:02<00:00, 50.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../../data/preprocessed/by_city-users_min_3_reviews/keywords_spacy/test/london-keywords.json\n",
      "Processing for london dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1849/1849 [00:36<00:00, 51.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../../data/preprocessed/by_city-users_min_3_reviews/keywords_spacy/dev/london-keywords.json\n",
      "Processing for phoenix train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216488/216488 [59:12<00:00, 60.94it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../../data/preprocessed/by_city-users_min_3_reviews/keywords_spacy/train/phoenix-keywords.json\n",
      "Processing for phoenix test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39812/39812 [10:50<00:00, 61.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../../data/preprocessed/by_city-users_min_3_reviews/keywords_spacy/test/phoenix-keywords.json\n",
      "Processing for phoenix dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13571/13571 [03:44<00:00, 60.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../../data/preprocessed/by_city-users_min_3_reviews/keywords_spacy/dev/phoenix-keywords.json\n",
      "Processing for pittsburgh train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73558/73558 [13:06:00<00:00,  1.56it/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../../data/preprocessed/by_city-users_min_3_reviews/keywords_spacy/train/pittsburgh-keywords.json\n",
      "Processing for pittsburgh test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14107/14107 [04:41<00:00, 50.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../../data/preprocessed/by_city-users_min_3_reviews/keywords_spacy/test/pittsburgh-keywords.json\n",
      "Processing for pittsburgh dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4784/4784 [01:28<00:00, 54.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../../data/preprocessed/by_city-users_min_3_reviews/keywords_spacy/dev/pittsburgh-keywords.json\n",
      "Processing for singapore train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10615/10615 [03:54<00:00, 45.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../../data/preprocessed/by_city-users_min_3_reviews/keywords_spacy/train/singapore-keywords.json\n",
      "Processing for singapore test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1556/1556 [00:32<00:00, 47.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../../data/preprocessed/by_city-users_min_3_reviews/keywords_spacy/test/singapore-keywords.json\n",
      "Processing for singapore dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 707/707 [00:14<00:00, 48.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../../data/preprocessed/by_city-users_min_3_reviews/keywords_spacy/dev/singapore-keywords.json\n"
     ]
    }
   ],
   "source": [
    "setname = 'test'\n",
    "CITIES = ['charlotte', 'edinburgh', 'lasvegas', 'london', 'phoenix', 'pittsburgh', 'singapore']\n",
    "sets = ['train', 'test', 'dev']\n",
    "for city in CITIES: \n",
    "    dt = pd.read_csv('../../../data/preprocessed/by_city-users_min_3_reviews/reviews/{}.csv'.format(city))\n",
    "    for setname in sets: \n",
    "        print(\"Processing for\", city, setname)\n",
    "        uids = load_split(city=city, setname=setname)\n",
    "        dt_set = dt[dt['user_id'].isin(uids)]\n",
    "        odir = '../../../data/preprocessed/by_city-users_min_3_reviews/keywords_spacy/' + setname\n",
    "        mkdir(odir)\n",
    "        extract_raw_keywords_for_reviews(dt_set, ofile=os.path.join(odir, city + '-keywords.json'), keep=['ADJ', 'NOUN', 'PROPN', 'VERB'], \n",
    "                                        overwrite=True, review2keyword_ofile=os.path.join(odir,city+\"-review2keywords.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
